# -*- coding: utf-8 -*-
"""cadim pneumonia pna

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WO6NfUcxbN_fhsAwK_nlBwZVfR7ziOdz
"""

# !pip install jarvis-md==0.0.1a14
# !pip install pyyaml==5.1
# !pip install tensorflow==2.1.0

import os, numpy as np, pandas as pd
import tensorflow as tf
from tensorflow import losses, optimizers
from tensorflow.keras import Input, Model, models, layers, metrics, backend
from tensorflow.keras.mixed_precision import experimental as mixed_precision
from jarvis.train import datasets, custom
from jarvis.train.client import Client
from jarvis.utils.general import overload, tools as jtools, gpus
from jarvis.utils.display import imshow

gpus.autoselect(1)

client = Client('/data/raw/covid_biomarker/data/ymls/client-uci-512.yml')
gen_train_all, gen_valid = client.create_generators()
inputs = client.get_inputs(Input)

xs, ys = next(gen_train_all)
for key, arr in xs.items():
    print('xs key: {} | shape = {}'.format(key.ljust(8), arr.shape))
for key, arr in ys.items():
    print('ys key: {} | shape = {}'.format(key.ljust(8), arr.shape))
client.load_data_in_memory()

conv3 = lambda x, filters : layers.Conv3D(kernel_size=(1, 3, 3), filters=filters, 
                                          strides=1, padding='same')(x)
conv1 = lambda x, filters : layers.Conv3D(kernel_size=1, filters=filters, 
                                          strides=1, padding='same')(x)
pool = lambda x : layers.AveragePooling3D(pool_size=(1, 2, 2),strides=(1, 2, 2),
                                          padding='same')(x)
norm = lambda x : layers.BatchNormalization()(x)
relu = lambda x : layers.LeakyReLU()(x)
concat = lambda a, b : layers.Concatenate()([a, b])
dense = lambda x, k : conv3(relu(norm(x)), filters=k)
bneck = lambda x, b : conv1(relu(norm(x)), filters=b)
trans = lambda x, b : pool(bneck(x, b))
conv3t = lambda x, filters : layers.Conv3DTranspose(filters, kernel_size=(1,2,2),
                                            strides=(1,2,2))(x)
convT = lambda x, filters : conv3t(relu(norm(x)), filters)

def dense_block(x, k=8, n=3, b=1, verbose=True):
    ds_layer = None
    for i in range(n):
        cc_layer = concat(cc_layer, ds_layer) if ds_layer is not None else x
        bn_layer = bneck(cc_layer, b * k) if i >= b else cc_layer
        ds_layer = dense(bn_layer, k)
        if verbose:
            print('Creating layer {:02d}: cc_layer = {}'.format(i, cc_layer.shape))
            print('Creating layer {:02d}: bn_layer = {}'.format(i, bn_layer.shape))
            print('Creating layer {:02d}: ds_layer = {}'.format(i, ds_layer.shape))    
    return concat(cc_layer, ds_layer)

dense_block_ = lambda x : dense_block(x, k=16, n=3, b=1)
b0 = conv3(inputs['dat'], filters=8)
b1 = pool(dense_block_(b0))
b2 = pool(dense_block_(b1))

dense_block_ = lambda x : dense_block(x, k=24, n=4, b=2)
b3 = trans(dense_block_(b2), 80)
b4 = trans(dense_block_(b3), 96)
b5 = trans(dense_block_(b4), 112)
b6 = dense_block_(b5)
p1 = layers.GlobalAveragePooling3D()(b6)
logits = {}
logits['ratio'] = layers.Dense(1, name='ratio')(p1)
model = Model(inputs=inputs, outputs=logits)
print(model.summary())

def dsc_soft(weights=None, scale=1.0, epsilon=0.01, cls=1):
    scale = float(scale)
    def dsc(y_true, y_pred):
        true = tf.cast(y_true[..., 0] == cls, tf.float32)
        pred = tf.nn.softmax(y_pred, axis=-1)[..., cls]
        if weights is not None:
            true = true * weights
            pred = pred * weights
        A = tf.math.reduce_sum(true * pred) * 2
        B = tf.math.reduce_sum(true) + tf.math.reduce_sum(pred) + epsilon
        return (1.0-(A / B)) * scale
    return dsc

def sce(weights=None, scale=1.0):
    scale=float(scale)
    loss = losses.SparseCategoricalCrossentropy(from_logits=True)
    def sce(y_true, y_pred):
        return loss(y_true=y_true, y_pred=y_pred, sample_weight=weights) * scale
    return sce

def happy_meal(alpha=5, beta=1, weights=None, epsilon=1, cls=1):
    l2 = sce(weights=weights, scale=alpha)
    l1 = dsc_soft(weights=None, scale=beta, epsilon=epsilon, cls=1)
    def calc_loss(y_true, y_pred):
        return l2(y_true, y_pred) + l1(y_true, y_pred)
    return calc_loss

model.compile(
    optimizer=optimizers.Adam(learning_rate=2e-4),
    loss={'ratio': 'mse' },
    metrics={'ratio': 'mae'},
    experimental_run_tf_function=False)

model.fit(
    x=gen_train_all, 
    steps_per_epoch=1000, 
    epochs=24,
    validation_data=gen_valid,
    validation_steps=250,
    validation_freq=1,
)


model.trainable=False
model.save('model.h5', overwrite=True, include_optimizer=False)

# new_model = tf.keras.models.load_model('./model.h5')
# print(new_model.summary())

